




This group will create a web interface for artists to annotate different places on 3D virtual models of their statues.  An artist should be able to point and click on a 3D model and write something about it, and those landmarks should be saved for people to examine later on their own. 
The main application will be the smokestack, but if the software is done well, the Berman museum is interested in adopting it for their own statues beyond that to help create virtual museum exhibits.  This will be pretty high impact and visible around campus.  If you are particularly excited about this project, please let me know ASAP so we can try to have you come visit Katie Merz, the smokestack artist, this Friday.


------------------------
Photogrammetry
* AJ Alvero
* Seraiah Kutai
This group will be responsible for creating a 3D model of the smokestack by using a process known as "photogrammetry," which is the ability to infer a 3D model from a bunch of pictures from different angles.  There is a lot of open source software to do this:
https://all3dp.com/1/best-photogrammetry-software/
but it's unclear what the best option is, so part of this group's work will be experimenting with different software to figure out what works best.  There's a good chance we'll have a bunch of frames from aerial drone footage that we can use for this purpose.

------------------------
Annotation Interface
* Ryan Whittaker
* Rachel Thornton
This group will work on an interface to implement user friendly "picking" on 3D models; that is, choosing specific points for a user to annotate and popping up text boxes to input and save information.


------------------------
Web Interface / Database Engineering
* Liz Dempsey
* Nina Flamiano
This group will be responsible for loading in 3D models from storage and incrementally streaming them into the display, as well as setting up a well-formatted web page to house everything.  For high resolution models, they should work to set up an interface of "progressive refinement"; that is, using higher detail models when more data is loaded / when the user is zoomed in further.  They will also work closely with the annotation group to save the inputted annotations to some database service.










Other ideas:
1) Flattening the smokestack and looking at everything on a sheet of paper
2) Some predefined zooming in and out to show how perspective factors into the work
3) Going back and forth between perspective viewing and orthographic viewing
4) Some animation/interface to help you explain the 6/8 timing
5) This one would be tougher, but something about taking apart and rearranging the shapes.  


